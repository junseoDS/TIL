{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W : [0.79807566 0.84303798 0.30548407], b : [0.07299533], loss : 395.1568633624608\n",
      "W : [ 1.07523275  1.12941768 -1.61509626], b : [-1.25309507], loss : 221.21560007220964\n",
      "W : [ 1.07526592  1.12942595 -1.61508268], b : [-1.25312616], loss : 221.21560006866997\n",
      "W : [ 1.07526592  1.12942595 -1.61508268], b : [-1.25312617], loss : 221.21560006866994\n",
      "W : [ 1.07526592  1.12942595 -1.61508268], b : [-1.25312617], loss : 221.21560006866994\n",
      "W : [ 1.07526592  1.12942595 -1.61508268], b : [-1.25312617], loss : 221.21560006866994\n",
      "W : [ 1.07526592  1.12942595 -1.61508268], b : [-1.25312617], loss : 221.21560006866994\n",
      "W : [ 1.07526592  1.12942595 -1.61508268], b : [-1.25312617], loss : 221.21560006866994\n",
      "W : [ 1.07526592  1.12942595 -1.61508268], b : [-1.25312617], loss : 221.21560006866994\n",
      "W : [ 1.07526592  1.12942595 -1.61508268], b : [-1.25312617], loss : 221.21560006866994\n"
     ]
    }
   ],
   "source": [
    "# Admission 예제\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# python 구현을 위한 수치미분함수가 필요!\n",
    "# 수치미분함수(for python)\n",
    "def numerical_derivative(f,x):\n",
    "    \n",
    "    # f : 미분하려고 하는 다변수 함수(loss 함수)\n",
    "    # x : 모든 값을 포함하는 numpy array => [W, b] \n",
    "    delta_x = 1e-4\n",
    "    derivative_x = np.zeros_like(x)    # [0 0]\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        \n",
    "        idx = it.multi_index   # 현재의 iterator의 index를 추출 => tuple형태로 나와요\n",
    "        \n",
    "        tmp = x[idx]     # 현재 index의 값을 잠시 보존.\n",
    "                         # delta_x를 이용한 값으로 ndarray를 수정한 후 편미분을 계산\n",
    "                         # 함수값을 계산한 후 원상복구를 해 줘야 다음 독립변수에\n",
    "                         # 대한 편미분을 정상적으로 수행할 수 있어요!\n",
    "        x[idx] = tmp + delta_x        \n",
    "        fx_plus_delta = f(x)    # f([1.00001, 2.0])   => f(x + delta_x)\n",
    "        \n",
    "\n",
    "        x[idx] = tmp - delta_x\n",
    "        fx_minus_delta = f(x)    # f([0.99999, 2.0])   => f(x - delta_x)\n",
    "        \n",
    "        derivative_x[idx] = (fx_plus_delta - fx_minus_delta) / (2 * delta_x)\n",
    "        \n",
    "        x[idx] = tmp\n",
    "        \n",
    "        it.iternext()\n",
    "        \n",
    "    return derivative_x\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd.read_csv('./data/admission.csv')\n",
    "# display(df)\n",
    "\n",
    "# 데이터 전처리\n",
    "# 1. 결측치 처리\n",
    "# print(df.isnull().sum())  # 결측치는 없어요!\n",
    "\n",
    "# 2. 이상치 처리\n",
    "# 이상치가 있는지 먼저 눈으로 확인 => boxplot을 이용\n",
    "# fig = plt.figure()\n",
    "# fig_gre = fig.add_subplot(1,3,1)\n",
    "# fig_gpa = fig.add_subplot(1,3,2)\n",
    "# fig_rank = fig.add_subplot(1,3,3)\n",
    "# fig_gre.boxplot(df['gre'])\n",
    "# fig_gpa.boxplot(df['gpa'])\n",
    "# fig_rank.boxplot(df['rank'])\n",
    "\n",
    "# fig.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# 이상치가 존재해요!!\n",
    "# Z-Score방식으로 이상치를 제거\n",
    "zscore_threshold = 2.0 # 2.0이하로 설정하는게 optimal\n",
    "\n",
    "for col in df.columns:\n",
    "    outlier = df[col][(np.abs(stats.zscore(df[col])) > zscore_threshold)]\n",
    "    df = df.loc[~df[col].isin(outlier)]\n",
    "    \n",
    "# display(df)   # 382 rows × 4 columns  \n",
    "\n",
    "# 이상치를 성공적으로 제거했으니 이제 Normalization(정규화)처리를 해 보아요!\n",
    "x_data = df.drop('admit', axis=1, inplace=False).values\n",
    "# print(x_data)\n",
    "t_data = df['admit'].values.reshape(-1,1)\n",
    "# print(t_data)\n",
    "\n",
    "# sklearn은 정규화하지 않은 데이터를 이용.\n",
    "# python구현과 tensorflow 구현은 데이터를 정규화해서 사용해야 해요!\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_x.fit(x_data)\n",
    "norm_x_data = scaler_x.transform(x_data)   # for python, tensorflow\n",
    "# print(norm_x_data)\n",
    "\n",
    "# python 구현부터 알아보아요!\n",
    "\n",
    "# Weight & bias\n",
    "W = np.random.rand(3,1)   # 3 : 행렬곱을 위해 입력데이터 x의 열의 수\n",
    "                          # 1 : 예측값이 t_data와 연산이 되야 되요. t_data의 \n",
    "                          #     column 수와 동일.\n",
    "b = np.random.rand(1)        \n",
    "\n",
    "# loss function\n",
    "def loss_func(input_obj):    # input_obj : [w1 w2 w3 b]\n",
    "    \n",
    "    input_W = input_obj[:-1].reshape(-1,1)\n",
    "    input_b = input_obj[-1:]\n",
    "    \n",
    "    delta = 1e-7    # log연산시 무한대로 발산하는것을 방지하기 위한 수치처리\n",
    "    \n",
    "    z = np.dot(norm_x_data,input_W) + input_b\n",
    "    y = 1 / ( 1 + np.exp(-1*z))\n",
    "    \n",
    "    # cross entropy\n",
    "    return -np.sum(t_data*np.log(y+delta) + (1-t_data)*np.log(1-y+delta))\n",
    "\n",
    "# learning rate 정의\n",
    "learning_rate = 1e-4\n",
    "\n",
    "for step in range(300000):\n",
    "    input_param = np.concatenate((W.ravel(), b.ravel()), axis=0) # [w1 w2 w3 b]\n",
    "    derivative_result = learning_rate * numerical_derivative(loss_func, input_param)\n",
    "    \n",
    "    W = W - derivative_result[:-1].reshape(-1,1)   # W 갱신(w1, w2, w3 갱신)\n",
    "    b = b - derivative_result[-1:]                 # b 갱신\n",
    "    \n",
    "    if step % 30000 == 0:\n",
    "        input_param = np.concatenate((W.ravel(), b.ravel()), axis=0)\n",
    "        print('W : {}, b : {}, loss : {}'.format(W.ravel(), b, loss_func(input_param)))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, array([[0.57333869]]))\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "def logistic_predict(x):\n",
    "    z = np.dot(x,W) + b\n",
    "    y = 1 / ( 1 + np.exp(-1*z))\n",
    "    \n",
    "    if y < 0.5:\n",
    "        result = 0\n",
    "    else:\n",
    "        result = 1\n",
    "    \n",
    "    return result, y\n",
    "\n",
    "my_score = np.array([600, 3.8, 1])\n",
    "scaled_my_score = scaler_x.transform(my_score.reshape(-1,3))\n",
    "result = logistic_predict(scaled_my_score)\n",
    "print(result)   # (1, array([[0.57333869]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] [[0.43740782 0.56259218]]\n"
     ]
    }
   ],
   "source": [
    "# sklearn 구현\n",
    "\n",
    "model = linear_model.LogisticRegression()\n",
    "\n",
    "model.fit(x_data, t_data.ravel())\n",
    "\n",
    "my_score = np.array([600, 3.8, 1]).reshape(-1,3)\n",
    "predict_result = model.predict(my_score)\n",
    "predict_proba = model.predict_proba(my_score)\n",
    "print(predict_result, predict_proba)   \n",
    "# [1] [[0.43740782 0.56259218]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\envs\\data_env\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W : [ 0.01384994 -2.3655026   0.2143435 ], b: [0.26223746], loss : 0.7318820357322693\n",
      "W : [ 0.12779304 -2.17954     0.11536565], b: [0.3080888], loss : 0.7116413116455078\n",
      "W : [ 0.20679197 -2.0301309  -0.00399886], b: [0.2982963], loss : 0.6972891688346863\n",
      "W : [ 0.27091944 -1.8978132  -0.12437461], b: [0.26982766], loss : 0.6849640607833862\n",
      "W : [ 0.32733616 -1.7749879  -0.2394275 ], b: [0.23548503], loss : 0.6740601062774658\n",
      "W : [ 0.3786337  -1.6589776  -0.34741113], b: [0.19956604], loss : 0.6643809080123901\n",
      "W : [ 0.42583543 -1.5493318  -0.44806904], b: [0.16349876], loss : 0.6558083891868591\n",
      "W : [ 0.46937534 -1.4447602  -0.541701  ], b: [0.12782905], loss : 0.6481893658638\n",
      "W : [ 0.5096828  -1.3458272  -0.62856746], b: [0.09268671], loss : 0.6414452195167542\n",
      "W : [ 0.5468554  -1.2513956  -0.70924497], b: [0.05813945], loss : 0.6354442834854126\n"
     ]
    }
   ],
   "source": [
    "# tensorflow 구현\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,3], dtype=tf.float32)\n",
    "T = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random.normal([3,1]), name='weight')\n",
    "b = tf.Variable(tf.random.normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "logit = tf.matmul(X,W) + b\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# loss func(cross entropy)\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,\n",
    "                                                              labels=T))\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-4).minimize(loss)\n",
    "\n",
    "# session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(300000):\n",
    "    \n",
    "    _, W_val, b_val, loss_val = sess.run([train,W,b,loss], \n",
    "                                         feed_dict={X:norm_x_data,\n",
    "                                                    T:t_data})\n",
    "    if step % 30000 == 0:\n",
    "        print('W : {}, b: {}, loss : {}'.format(W_val.ravel(),b_val,loss_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.3434503]]\n"
     ]
    }
   ],
   "source": [
    "my_score = np.array([600, 3.8, 1])\n",
    "scaled_my_score = scaler_x.transform(my_score.reshape(-1,3))\n",
    "result = sess.run(H, feed_dict={X:scaled_my_score})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_env] *",
   "language": "python",
   "name": "conda-env-data_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
