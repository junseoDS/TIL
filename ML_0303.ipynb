{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W : [0.13017394 0.97008634 0.0926367 ], b : [0.66002466], loss : 428.53800881681093\n",
      "W : [ 1.07515469  1.12942495 -1.61511996], b : [-1.25304261], loss : 221.2156001000295\n",
      "W : [ 1.07526591  1.12942595 -1.61508269], b : [-1.25312616], loss : 221.21560006866997\n",
      "W : [ 1.07526592  1.12942595 -1.61508268], b : [-1.25312617], loss : 221.21560006866997\n",
      "W : [ 1.07526592  1.12942595 -1.61508268], b : [-1.25312617], loss : 221.21560006866997\n",
      "W : [ 1.07526592  1.12942595 -1.61508268], b : [-1.25312617], loss : 221.21560006866997\n",
      "W : [ 1.07526592  1.12942595 -1.61508268], b : [-1.25312617], loss : 221.21560006866997\n",
      "W : [ 1.07526592  1.12942595 -1.61508268], b : [-1.25312617], loss : 221.21560006866997\n",
      "W : [ 1.07526592  1.12942595 -1.61508268], b : [-1.25312617], loss : 221.21560006866997\n",
      "W : [ 1.07526592  1.12942595 -1.61508268], b : [-1.25312617], loss : 221.21560006866997\n"
     ]
    }
   ],
   "source": [
    "# Admission 예제\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# python 구현을 위한 수치미분함수가 필요!\n",
    "# 수치미분함수(for python)\n",
    "def numerical_derivative(f,x):\n",
    "    \n",
    "    # f : 미분하려고 하는 다변수 함수(loss 함수)\n",
    "    # x : 모든 값을 포함하는 numpy array => [W, b] \n",
    "    delta_x = 1e-4\n",
    "    derivative_x = np.zeros_like(x)    # [0 0]\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        \n",
    "        idx = it.multi_index   # 현재의 iterator의 index를 추출 => tuple형태로 나와요\n",
    "        \n",
    "        tmp = x[idx]     # 현재 index의 값을 잠시 보존.\n",
    "                         # delta_x를 이용한 값으로 ndarray를 수정한 후 편미분을 계산\n",
    "                         # 함수값을 계산한 후 원상복구를 해 줘야 다음 독립변수에\n",
    "                         # 대한 편미분을 정상적으로 수행할 수 있어요!\n",
    "        x[idx] = tmp + delta_x        \n",
    "        fx_plus_delta = f(x)    # f([1.00001, 2.0])   => f(x + delta_x)\n",
    "        \n",
    "\n",
    "        x[idx] = tmp - delta_x\n",
    "        fx_minus_delta = f(x)    # f([0.99999, 2.0])   => f(x - delta_x)\n",
    "        \n",
    "        derivative_x[idx] = (fx_plus_delta - fx_minus_delta) / (2 * delta_x)\n",
    "        \n",
    "        x[idx] = tmp\n",
    "        \n",
    "        it.iternext()\n",
    "        \n",
    "    return derivative_x\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd.read_csv('./data/admission.csv')\n",
    "# display(df)\n",
    "\n",
    "# 데이터 전처리\n",
    "# 1. 결측치 처리\n",
    "# print(df.isnull().sum())  # 결측치는 없어요!\n",
    "\n",
    "# 2. 이상치 처리\n",
    "# 이상치가 있는지 먼저 눈으로 확인 => boxplot을 이용\n",
    "# fig = plt.figure()\n",
    "# fig_gre = fig.add_subplot(1,3,1)\n",
    "# fig_gpa = fig.add_subplot(1,3,2)\n",
    "# fig_rank = fig.add_subplot(1,3,3)\n",
    "# fig_gre.boxplot(df['gre'])\n",
    "# fig_gpa.boxplot(df['gpa'])\n",
    "# fig_rank.boxplot(df['rank'])\n",
    "\n",
    "# fig.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# 이상치가 존재해요!!\n",
    "# Z-Score방식으로 이상치를 제거\n",
    "zscore_threshold = 2.0 # 2.0이하로 설정하는게 optimal\n",
    "\n",
    "for col in df.columns:\n",
    "    outlier = df[col][(np.abs(stats.zscore(df[col])) > zscore_threshold)]\n",
    "    df = df.loc[~df[col].isin(outlier)]\n",
    "    \n",
    "# display(df)   # 382 rows × 4 columns  \n",
    "\n",
    "# 이상치를 성공적으로 제거했으니 이제 Normalization(정규화)처리를 해 보아요!\n",
    "x_data = df.drop('admit', axis=1, inplace=False).values\n",
    "# print(x_data)\n",
    "t_data = df['admit'].values.reshape(-1,1)\n",
    "# print(t_data)\n",
    "\n",
    "# sklearn은 정규화하지 않은 데이터를 이용.\n",
    "# python구현과 tensorflow 구현은 데이터를 정규화해서 사용해야 해요!\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_x.fit(x_data)\n",
    "norm_x_data = scaler_x.transform(x_data)   # for python, tensorflow\n",
    "# print(norm_x_data)\n",
    "\n",
    "# python 구현부터 알아보아요!\n",
    "\n",
    "# Weight & bias\n",
    "W = np.random.rand(3,1)   # 3 : 행렬곱을 위해 입력데이터 x의 열의 수\n",
    "                          # 1 : 예측값이 t_data와 연산이 되야 되요. t_data의 \n",
    "                          #     column 수와 동일.\n",
    "b = np.random.rand(1)        \n",
    "\n",
    "# loss function\n",
    "def loss_func(input_obj):    # input_obj : [w1 w2 w3 b]\n",
    "    \n",
    "    input_W = input_obj[:-1].reshape(-1,1)\n",
    "    input_b = input_obj[-1:]\n",
    "    \n",
    "    delta = 1e-7    # log연산시 무한대로 발산하는것을 방지하기 위한 수치처리\n",
    "    \n",
    "    z = np.dot(norm_x_data,input_W) + input_b\n",
    "    y = 1 / ( 1 + np.exp(-1*z))\n",
    "    \n",
    "    # cross entropy\n",
    "    return -np.sum(t_data*np.log(y+delta) + (1-t_data)*np.log(1-y+delta))\n",
    "\n",
    "# learning rate 정의\n",
    "learning_rate = 1e-4\n",
    "\n",
    "for step in range(300000):\n",
    "    input_param = np.concatenate((W.ravel(), b.ravel()), axis=0) # [w1 w2 w3 b]\n",
    "    derivative_result = learning_rate * numerical_derivative(loss_func, input_param)\n",
    "    \n",
    "    W = W - derivative_result[:-1].reshape(-1,1)   # W 갱신(w1, w2, w3 갱신)\n",
    "    b = b - derivative_result[-1:]                 # b 갱신\n",
    "    \n",
    "    if step % 30000 == 0:\n",
    "        input_param = np.concatenate((W.ravel(), b.ravel()), axis=0)\n",
    "        print('W : {}, b : {}, loss : {}'.format(W.ravel(), b, loss_func(input_param)))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, array([[0.57333869]]))\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "def logistic_predict(x):\n",
    "    z = np.dot(x,W) + b\n",
    "    y = 1 / ( 1 + np.exp(-1*z))\n",
    "    \n",
    "    if y < 0.5:\n",
    "        result = 0\n",
    "    else:\n",
    "        result = 1\n",
    "    \n",
    "    return result, y\n",
    "\n",
    "my_score = np.array([600, 3.8, 1])\n",
    "scaled_my_score = scaler_x.transform(my_score.reshape(-1,3))\n",
    "result = logistic_predict(scaled_my_score)\n",
    "print(result)   # (1, array([[0.57333869]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] [[0.43740782 0.56259218]]\n"
     ]
    }
   ],
   "source": [
    "# sklearn 구현\n",
    "\n",
    "model = linear_model.LogisticRegression()\n",
    "\n",
    "model.fit(x_data, t_data.ravel())\n",
    "\n",
    "my_score = np.array([600, 3.8, 1]).reshape(-1,3)\n",
    "predict_result = model.predict(my_score)\n",
    "predict_proba = model.predict_proba(my_score)\n",
    "print(predict_result, predict_proba)   \n",
    "# [1] [[0.43740782 0.56259218]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W : [ 0.10800714  0.96114427 -1.0171901 ], b: [-0.8140015], loss : 0.5898090600967407\n",
      "W : [ 0.13135205  0.96304756 -1.0655267 ], b: [-0.84222215], loss : 0.5885522365570068\n",
      "W : [ 0.16077657  0.97238153 -1.1050339 ], b: [-0.8568901], loss : 0.5876408815383911\n",
      "W : [ 0.19109291  0.98311037 -1.1403624 ], b: [-0.86737126], loss : 0.5868446826934814\n",
      "W : [ 0.22053573  0.9938392  -1.1725489 ], b: [-0.87631196], loss : 0.5861404538154602\n",
      "W : [ 0.24891227  1.004568   -1.2026378 ], b: [-0.88525265], loss : 0.5855053663253784\n",
      "W : [ 0.27609363  1.0152968  -1.231248  ], b: [-0.89419335], loss : 0.5849290490150452\n",
      "W : [ 0.30225462  1.0235343  -1.2573569 ], b: [-0.9023328], loss : 0.5844234824180603\n",
      "W : [ 0.32741132  1.0306869  -1.2823908 ], b: [-0.9095066], loss : 0.583966851234436\n",
      "W : [ 0.35141513  1.0378394  -1.3054036 ], b: [-0.9166592], loss : 0.5835583806037903\n"
     ]
    }
   ],
   "source": [
    "# tensorflow 구현\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,3], dtype=tf.float32)\n",
    "T = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random.normal([3,1]), name='weight')\n",
    "b = tf.Variable(tf.random.normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "logit = tf.matmul(X,W) + b\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# loss func(cross entropy)\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,\n",
    "                                                              labels=T))\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-4).minimize(loss)\n",
    "\n",
    "# session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(300000):\n",
    "    \n",
    "    _, W_val, b_val, loss_val = sess.run([train,W,b,loss], \n",
    "                                         feed_dict={X:norm_x_data,\n",
    "                                                    T:t_data})\n",
    "    if step % 30000 == 0:\n",
    "        print('W : {}, b: {}, loss : {}'.format(W_val.ravel(),b_val,loss_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.54253036]]\n"
     ]
    }
   ],
   "source": [
    "my_score = np.array([600, 3.8, 1])\n",
    "scaled_my_score = scaler_x.transform(my_score.reshape(-1,3))\n",
    "result = sess.run(H, feed_dict={X:scaled_my_score})\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_env] *",
   "language": "python",
   "name": "conda-env-data_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
