{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "median-activation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "[[-1.5028942   0.031176  ]\n",
      " [ 0.13045956  1.2824854 ]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random.normal([2,2], dtype=tf.float32))\n",
    "\n",
    "# 1.15버전에서는\n",
    "# 1. session이 있어야 해요!\n",
    "# 2. sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 2.x버전에서는\n",
    "print(W.numpy())   # Eager Execution (즉시 실행모드)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "noble-police",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결과는 : 120\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = 50\n",
    "b = 70\n",
    "\n",
    "def my_sum(x,y):\n",
    "    \n",
    "    t1 = tf.convert_to_tensor(x)\n",
    "    t2 = tf.convert_to_tensor(y)\n",
    "    \n",
    "    return t1 + t2\n",
    "\n",
    "result = my_sum(a,b)\n",
    "\n",
    "print(\"결과는 : {}\".format(result.numpy()))\n",
    "\n",
    "# placeholder를 더이상 사용하지 않아요!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-environment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras를 이용한 코드(Sample)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "model = Sequential()   # model 생성\n",
    "model.add(Flatten(input_shape=(1,)))\n",
    "model.add(Dense(1, activation='sigmoid' ))\n",
    "\n",
    "model.compile(optimizers=SGD(learning_rate=1e-3),\n",
    "              loss='mse')\n",
    "\n",
    "model.fit(x_data_train,\n",
    "          t_data_train,\n",
    "          epochs=10,\n",
    "          batch_size=200,\n",
    "          validation_split=0.2)\n",
    "\n",
    "model.evaluate(x_data_test,t_data_test)\n",
    "model.predict(x_data_predict)\n",
    "model.save('./myModel.h5')\n",
    "model = tf.keras.models.load_model('./myModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-athens",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "# Ozone data(ozone.csv)를 이용해서 학습 후 예측해 보아요!\n",
    "# sklearn과 tensorflow 2.x버전으로 구현.\n",
    "# 이번에는 결측치와 이상치 처리를 단순히 삭제하지 않고 적절한 값으로\n",
    "# 대치해서 학습하도록 해요!\n",
    "\n",
    "# 결측치 처리부터 알아보아요!\n",
    "# 데이터 분석 초기 단계, 예측 프로젝트 초기 단계에서 가장 힘든 일 중 \n",
    "# 하나가 결측치 처리!\n",
    "# 결측치가 전체 데이터에서 비중이 얼마나 되는가?\n",
    "# 결측치가 독립변수인지 종속변수인지 이것도 고민의 대상\n",
    "\n",
    "# 결측치 처리 방법은 크게 2가지 있어요!\n",
    "# 1. Deletion(결측치 제거)\n",
    "#    결측치 제거안에서도 Listwise 삭제방식과 Pairwise 삭제방식이 있어요!\n",
    "#    손쉽게 접근하는 방법이 Listwise 삭제 방식\n",
    "#    => NaN이 존재하면 행 자체를 삭제\n",
    "#    => NaN을 제외한 다른 의미있는 데이터가 같이 삭제된다는게 문제\n",
    "#    => 데이터가 충분히 많고 NaN의 빈도가 상대적으로 작을경우 최상의 방법\n",
    "#    => Pairwise는 의미있는 데이터가 삭제되는걸 막기 위해 행 전체를 삭제하지\n",
    "#    => 않아요. 그 값만 모든 처리에서 제외!(오히려 문제가 발생할 여지가 있어요!)\n",
    "# 2. Imputation(결측치 보간)\n",
    "#    결측치 보간 기법에는 다시 크게 2가지 방식이 있어요!\n",
    "#    평균화 기법, 예측기법이 있어요!\n",
    "#    평균화 기법 => 평균(mean), 중앙값(median), 최빈값(mode)\n",
    "#                   장점 : 쉽고 빨라요. 단점 : 통계분석에 영향을 많이 미쳐요.\n",
    "#    예측기법은 결측치들이 완전히 무작위적으로 관찰되지 않았다는 것을\n",
    "#    가정해요! => 종속변수와 같은 경우를 의미해요!\n",
    "#    우리예제에서는 머신러닝 기법 중 KNN을 이용해서 Imputation을 진행할\n",
    "#    꺼예요. 일반적으로 평균화 기법보다는 조금 더 나은 결측치 보간을 할 수 있어요!\n",
    "\n",
    "#    그러면 결측치 보간을 위해 KNN부터 알아보아요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "flexible-presence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9845\n",
      "0.998\n"
     ]
    }
   ],
   "source": [
    "#  KNN을 알아보았어요!!\n",
    "#  sklearn을 이용해서 BMI예제를 가지고 Logistic Regression을 이용한\n",
    "#  accuracy값과 KNN을 이용한 accuracy값을 비교해 보아요!\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd.read_csv('./data/bmi/bmi.csv', skiprows=3)\n",
    "\n",
    "# data split\n",
    "x_data_train, x_data_test, t_data_train, t_data_test = \\\n",
    "train_test_split(df[['height','weight']], df['label'],\n",
    "                test_size=0.3,\n",
    "                random_state=0)\n",
    "\n",
    "# Normalization\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_data_train)\n",
    "x_data_train_norm = scaler.transform(x_data_train)\n",
    "x_data_test_norm = scaler.transform(x_data_test)\n",
    "\n",
    "# LogisiticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(x_data_train_norm,t_data_train)\n",
    "print(model.score(x_data_test_norm,t_data_test))   # 0.9845\n",
    "\n",
    "# KNeighborsClassifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_classifier.fit(x_data_train_norm,t_data_train)\n",
    "print(knn_classifier.score(x_data_test_norm,t_data_test))   # 0.998"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_env_tf2] *",
   "language": "python",
   "name": "conda-env-data_env_tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
